{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training-TFIDF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aleman778/Chatbot-Gamma/blob/master/notebooks/training_TFIDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhLF4TjO2WjW",
        "colab_type": "text"
      },
      "source": [
        "# Training Using TFIDF Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c0p_oMbwxaT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 982
        },
        "outputId": "711434f8-a659-424e-dc59-5558998d7095"
      },
      "source": [
        "!rm -r chatbot-gamma\n",
        "!git clone https://github.com/Aleman778/Chatbot-Gamma chatbot-gamma\n",
        "!pip install transformers\n",
        "!pip install -e chatbot-gamma\n",
        "import sys\n",
        "sys.path.append('/content/chatbot-gamma')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'chatbot-gamma': No such file or directory\n",
            "Cloning into 'chatbot-gamma'...\n",
            "remote: Enumerating objects: 94, done.\u001b[K\n",
            "remote: Counting objects: 100% (94/94), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 94 (delta 34), reused 72 (delta 17), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (94/94), done.\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/1a/364556102943cacde1ee00fdcae3b1615b39e52649eddbf54953e5b144c9/transformers-2.2.1-py3-none-any.whl (364kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 37.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 64.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.36)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/db/4b29a0adec5881542cd81cb5d1929b5c0787003c5740b3c921e627d9c2e5/regex-2019.12.9.tar.gz (669kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 60.3MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 63.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.36 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.36)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: regex, sacremoses\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.12.9-cp36-cp36m-linux_x86_64.whl size=609184 sha256=768c708355bba1f6d460b64e6abef8df4ace4054df878218582805c40a4f6701\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/fb/b3/a89169557229468c49ca64f6839418f22461f6ee0a74f342b1\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=472b8c11a2d4d774fe52f81e11bc3a5b78ca9a680d6cb9547094ec1dc7981b04\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built regex sacremoses\n",
            "Installing collected packages: sentencepiece, regex, sacremoses, transformers\n",
            "Successfully installed regex-2019.12.9 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.2.1\n",
            "Obtaining file:///content/chatbot-gamma\n",
            "Installing collected packages: cbgamma\n",
            "  Running setup.py develop for cbgamma\n",
            "Successfully installed cbgamma\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXA2IzcX18KL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        },
        "outputId": "a92cc5b5-0ccd-4c5d-c630-fa123e17d1c4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from cbgamma.datasets import AmazonReviews\n",
        "from cbgamma.transforms import ToTfidf\n",
        "    \n",
        "\n",
        "def train():\n",
        "    train_loss = 0\n",
        "    for batch_nr, (data, target) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        target = to_onehot(target)\n",
        "        \n",
        "        prediction = network(data)\n",
        "        loss = loss_function(prediction, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss += loss.item()\n",
        "        print(\n",
        "            '\\rEpoch {} [{}/{}] - Loss: {}'.format(\n",
        "                current_epoch+1, batch_nr+1, len(train_loader), loss\n",
        "            ),\n",
        "            end=''\n",
        "        )\n",
        "        train_loss /= len(train_loader)\n",
        "        losses.append(train_loss)\n",
        "        \n",
        "    \n",
        "def validate():\n",
        "    validation_loss = 0\n",
        "    for batch_nr, (data, target) in enumerate(validation_loader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        target = to_onehot(target)\n",
        "        \n",
        "        prediction = network(data)\n",
        "        loss = loss_function(prediction, target)\n",
        "        loss.backward()\n",
        "        validation_loss += loss.item()\n",
        "        print(\n",
        "            '\\rEpoch {} [{}/{}] - Validation: {}'.format(\n",
        "                current_epoch+1, batch_nr+1, len(validation_loader), loss\n",
        "            ),\n",
        "            end=''\n",
        "        )\n",
        "        validation_loss /= len(validation_loader)\n",
        "        validation_losses.append(validation_loss)\n",
        "        \n",
        "    return validation_loss\n",
        "\n",
        "\n",
        "def chat():\n",
        "    while True:\n",
        "        text = input('>')\n",
        "        if text == \"exit()\":\n",
        "            break;\n",
        "        data = train_dataset.vectorizer([text])\n",
        "        data = torch.from_numpy(np.array(data)).type(torch.FloatTensor)\n",
        "        data = data.to(device)\n",
        "        prediction = network(data)\n",
        "        sentiment = \"positive\" if torch.argmax(prediction) == 1 else \"negative\"\n",
        "        print(\"Text: {}\\nPrediction: {}\\nSentiment: {}\\n\".format(text, prediction, sentiment))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    vectorizer = ToTfidf()\n",
        "    train_dataset = AmazonReviews('./', train=True, vectorizer=vectorizer, download=True, stopwords=True)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=50, shuffle=False)\n",
        "    \n",
        "    validation_dataset = AmazonReviews('./', train=False, vectorizer=vectorizer, download=True, stopwords=True)\n",
        "    validation_loader = DataLoader(validation_dataset, batch_size=50, shuffle=False)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
        "    print(\"Training on:\", device_name)\n",
        "    \n",
        "    network = nn.Sequential(\n",
        "        nn.Linear(5199,2),\n",
        "    )\n",
        "\n",
        "    network.to(device)\n",
        "\n",
        "    optimizer = optim.SGD(network.parameters(), lr=0.05)\n",
        "    loss_function = nn.MSELoss()\n",
        "    to_onehot = nn.Embedding(2, 2) \n",
        "    to_onehot.weight.data = torch.eye(2)\n",
        "    to_onehot.to(device)\n",
        "    epochs = 500\n",
        "    current_epoch = 0\n",
        "    losses = []\n",
        "    validation_losses = []\n",
        "    \n",
        "    while current_epoch < epochs:\n",
        "        train()\n",
        "        loss = validate()\n",
        "        current_epoch += 1\n",
        "\n",
        "    print(\"\\nDone training.\")\n",
        "    chat()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://drive.google.com/uc?export=download&id=1BvKLnZU3A8d-JSR3o1-KZeKSHaDqlepN to ./AmazonReviews/raw/amazon_cells_labelled.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "65536it [00:01, 57553.96it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processing...\n",
            "Processing...\n",
            "Done!\n",
            "Training on: Tesla T4\n",
            "Epoch 500 [2/2] - Validation: 0.04709818586707115\n",
            "Done training.\n",
            ">This is a good product, can strongly recommend it\n",
            "Text: This is a good product, can strongly recommend it\n",
            "Prediction: tensor([[-0.1503,  1.1517]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "Sentiment: positive\n",
            "\n",
            ">There is something wrong with my product i received\n",
            "Text: There is something wrong with my product i received\n",
            "Prediction: tensor([[0.6407, 0.3781]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "Sentiment: negative\n",
            "\n",
            ">Test1234\n",
            "Text: Test1234\n",
            "Prediction: tensor([[0.5865, 0.4090]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "Sentiment: negative\n",
            "\n",
            ">Apples\n",
            "Text: Apples\n",
            "Prediction: tensor([[0.5865, 0.4090]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "Sentiment: negative\n",
            "\n",
            ">Bad\n",
            "Text: Bad\n",
            "Prediction: tensor([[ 1.1641, -0.1576]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "Sentiment: negative\n",
            "\n",
            ">Good\n",
            "Text: Good\n",
            "Prediction: tensor([[-0.3495,  1.3492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "Sentiment: positive\n",
            "\n",
            ">I want to like this product, but it it is not worth the price\n",
            "Text: I want to like this product, but it it is not worth the price\n",
            "Prediction: tensor([[0.3048, 0.6973]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "Sentiment: positive\n",
            "\n",
            ">This is good and bad\n",
            "Text: This is good and bad\n",
            "Prediction: tensor([[0.5183, 0.4886]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "Sentiment: negative\n",
            "\n",
            ">exit()\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}